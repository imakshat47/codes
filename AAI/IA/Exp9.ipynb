{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMsX9ycUqOMFdsvFOM9Gs08"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Experiment 9**"],"metadata":{"id":"j6My9vpH3xtD"}},{"cell_type":"markdown","source":["## **Aim: Reinforcement Learning using Q-Learning**"],"metadata":{"id":"3xkyLl14z-rw"}},{"cell_type":"markdown","source":["## **Theory**"],"metadata":{"id":"QL5GDg1K4Iln"}},{"cell_type":"markdown","source":["**Q-Learning in Reinforcement Learning**\n","\n","1. Introduction:\n","\n","\n","Q-Learning is a type of model-free Reinforcement Learning (RL) algorithm that allows an agent to learn how to act optimally in a given environment by interacting with it. It learns a Q-value (action-value function) which estimates the expected future reward for taking a particular action in a given state.\n","\n","2. Exploration vs Exploitation:\n","\n","\n","\t•\tExploration helps the agent try new actions to discover rewards.\n","\t•\tExploitation uses the current best-known action.\n","\t•\tControlled by \\epsilon-greedy strategy:\n","\t•\tWith probability \\epsilon, choose a random action.\n","\t•\tOtherwise, choose the best-known action.\n","\n","3. GridWorld Example:\n","In the 4x4 grid example:\n","\n","\n","\t•\tThe agent starts at (0,0) and learns to reach the goal at (3,3).\n","\t•\tTrap at (1,2) gives heavy penalty (-10).\n","\t•\tRewards guide the agent to learn the shortest and safest path using Q-values."],"metadata":{"id":"QnZmjSD1E6dJ"}},{"cell_type":"markdown","source":["## **Code & Output**"],"metadata":{"id":"1_cD50233TVe"}},{"cell_type":"code","source":["import numpy as np\n","import random\n","\n","# Environment setup\n","grid_size = 4\n","actions = ['up', 'down', 'left', 'right']\n","q_table = np.zeros((grid_size, grid_size, len(actions)))\n","\n","# Rewards: Goal at (3,3), Trap at (1,2)\n","rewards = np.full((grid_size, grid_size), -1)\n","rewards[3][3] = 10   # Goal\n","rewards[1][2] = -10  # Trap\n","\n","# Parameters\n","alpha = 0.1      # Learning rate\n","gamma = 0.9      # Discount factor\n","epsilon = 0.2    # Exploration rate\n","episodes = 500\n","\n","# Helper function to move the agent\n","def take_action(state, action):\n","    x, y = state\n","    if action == 'up': x = max(0, x - 1)\n","    elif action == 'down': x = min(grid_size - 1, x + 1)\n","    elif action == 'left': y = max(0, y - 1)\n","    elif action == 'right': y = min(grid_size - 1, y + 1)\n","    return (x, y)\n","\n","# Training\n","for episode in range(episodes):\n","    state = (0, 0)\n","    while state != (3, 3):  # until goal is reached\n","        if random.uniform(0, 1) < epsilon:\n","            action_index = random.randint(0, len(actions) - 1)\n","        else:\n","            action_index = np.argmax(q_table[state[0], state[1]])\n","\n","        action = actions[action_index]\n","        next_state = take_action(state, action)\n","        reward = rewards[next_state]\n","\n","        # Q-learning formula\n","        old_q = q_table[state[0], state[1], action_index]\n","        next_max = np.max(q_table[next_state[0], next_state[1]])\n","        q_table[state[0], state[1], action_index] = old_q + alpha * (reward + gamma * next_max - old_q)\n","\n","        if next_state == (1, 2):  # Trap\n","            break\n","        state = next_state\n","\n","# Display learned Q-values\n","print(\"Learned Q-Table:\")\n","for i in range(grid_size):\n","    for j in range(grid_size):\n","        print(f\"State ({i},{j}): {q_table[i, j]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"ecebuuo5E5sW","executionInfo":{"status":"ok","timestamp":1744856089099,"user_tz":-330,"elapsed":88,"user":{"displayName":"Yash Agrawal","userId":"00981640824773720604"}},"outputId":"d2869fd8-71c0-42f0-ea45-d015f34de33a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Learned Q-Table:\n","State (0,0): [0.24543278 1.8098     0.36287732 1.4591348 ]\n","State (0,1): [-1.09201107  3.06784721 -0.729901   -1.65522723]\n","State (0,2): [-1.04450422 -3.439      -1.07953973 -0.95013658]\n","State (0,3): [-0.4900995   0.79039566 -0.55713484 -0.58001377]\n","State (1,0): [0.1246892  2.56065608 1.55014939 3.122     ]\n","State (1,1): [ 0.84249298  4.58        1.32198696 -9.57608842]\n","State (1,2): [0. 0. 0. 0.]\n","State (1,3): [-0.20791     5.25875108 -3.439       0.19456136]\n","State (2,0): [-0.49972567 -0.44585305 -0.52413971  4.54636841]\n","State (2,1): [2.95219901 6.2        2.33562076 4.81198969]\n","State (2,2): [-2.71        0.57666524  1.11142402  7.79455787]\n","State (2,3): [1.29342602 9.96618608 1.05388684 1.67746402]\n","State (3,0): [-0.32803325  0.10361247  0.90974524  5.8400335 ]\n","State (3,1): [4.2783026  5.73477138 3.36764273 8.        ]\n","State (3,2): [ 4.3460774   7.47536036  5.77465145 10.        ]\n","State (3,3): [0. 0. 0. 0.]\n"]}]}]}