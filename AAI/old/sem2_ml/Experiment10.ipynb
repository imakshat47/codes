{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvfoHHiJeTKa"
   },
   "source": [
    "## **Importing Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5955,
     "status": "ok",
     "timestamp": 1745122198925,
     "user": {
      "displayName": "Yash Agrawal",
      "userId": "00981640824773720604"
     },
     "user_tz": -330
    },
    "id": "EWJrvpLednQi",
    "outputId": "90341c74-0415-4b40-f859-5c2c5a76c604"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /kaggle/input/dog-and-cat-classification-dataset\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"bhavikjikadara/dog-and-cat-classification-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNmbjUSKZIud"
   },
   "source": [
    "## **Question 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 283279,
     "status": "ok",
     "timestamp": 1745122482209,
     "user": {
      "displayName": "Yash Agrawal",
      "userId": "00981640824773720604"
     },
     "user_tz": -330
    },
    "id": "6gD1D7XVdxoD",
    "outputId": "ce2ed127-fe12-4ebd-d87c-884a9dc96537"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:949: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.33      0.42      2515\n",
      "           1       0.53      0.77      0.63      2485\n",
      "\n",
      "    accuracy                           0.55      5000\n",
      "   macro avg       0.56      0.55      0.53      5000\n",
      "weighted avg       0.56      0.55      0.53      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Function to extract grayscale image features and flatten them\n",
    "def load_image_data(image_dir, label, image_size=(64, 64)):\n",
    "    data, labels = [], []\n",
    "    for file in os.listdir(image_dir):\n",
    "        if file.endswith('.jpg'):\n",
    "            image_path = os.path.join(image_dir, file)\n",
    "            try:\n",
    "                img = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "                img = img.resize(image_size)\n",
    "                data.append(np.array(img).flatten())      # Flatten the image\n",
    "                labels.append(label)\n",
    "            except Exception as e:\n",
    "                pass  # Skip corrupted or unreadable images\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Set dataset directories\n",
    "cat_path = \"/kaggle/input/dog-and-cat-classification-dataset/PetImages/Cat\"\n",
    "dog_path = \"/kaggle/input/dog-and-cat-classification-dataset/PetImages/Dog\"\n",
    "\n",
    "# Load and label the data\n",
    "cat_data, cat_labels = load_image_data(cat_path, label=0)\n",
    "dog_data, dog_labels = load_image_data(dog_path, label=1)\n",
    "\n",
    "# Combine cat and dog data\n",
    "X = np.vstack((cat_data, dog_data))\n",
    "y = np.concatenate((cat_labels, dog_labels))\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define and train a feed-forward neural network\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', max_iter=100, random_state=1)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate model performance\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0CTpbktfjhE"
   },
   "source": [
    "**Question:** What were the limitations of FFNNs with high-dimensional image inputs?\n",
    "\n",
    "**Answer:**\n",
    "Feedforward Neural Networks (FFNNs) have several limitations when applied to high-dimensional image inputs. One of the primary issues is the large number of parameters involved. Since each pixel in an image becomes an input feature, even a relatively small grayscale image of size 64×64 results in 4,096 input nodes. When these are connected to hidden layers in a fully connected manner, the number of weights increases exponentially, making the model computationally expensive and highly prone to overfitting, especially when the available training data is limited. Additionally, FFNNs lack spatial awareness—they treat each pixel as an independent feature and do not account for local patterns or the spatial relationships between pixels. This means they are unable to capture important visual features like edges, textures, or shapes that are critical for understanding image content. Moreover, FFNNs do not have the property of translation invariance; they cannot recognize objects in varying positions across the image, which is essential in image classification tasks. They also do not utilize parameter sharing, which leads to redundant computations and inefficient use of model capacity. Because of these limitations, FFNNs generally perform poorly on complex visual tasks compared to Convolutional Neural Networks (CNNs), which are specifically designed to handle image data by exploiting spatial hierarchies, local connectivity, and weight sharing. Therefore, while FFNNs may still be useful for simple or low-dimensional image tasks, CNNs are far more suitable and effective for real-world image processing applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Au2w4aOZgSQq"
   },
   "source": [
    "## **Question 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 780
    },
    "executionInfo": {
     "elapsed": 509051,
     "status": "ok",
     "timestamp": 1745122991250,
     "user": {
      "displayName": "Yash Agrawal",
      "userId": "00981640824773720604"
     },
     "user_tz": -330
    },
    "id": "IppW40ibeoGX",
    "outputId": "2b1ddb7f-5cb7-464e-e372-0691c2539372"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 4998 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12288</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,572,992</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12288\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,572,992\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,581,313</span> (6.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,581,313\u001b[0m (6.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,581,313</span> (6.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,581,313\u001b[0m (6.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 34/625\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 62ms/step - accuracy: 0.5717 - loss: 1.4449"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:949: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 75ms/step - accuracy: 0.5483 - loss: 0.8527 - val_accuracy: 0.5348 - val_loss: 0.6945\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.5827 - loss: 0.6755 - val_accuracy: 0.6066 - val_loss: 0.6582\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 76ms/step - accuracy: 0.5852 - loss: 0.6672 - val_accuracy: 0.6116 - val_loss: 0.6539\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 75ms/step - accuracy: 0.6106 - loss: 0.6547 - val_accuracy: 0.5830 - val_loss: 0.6803\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 73ms/step - accuracy: 0.6128 - loss: 0.6514 - val_accuracy: 0.5878 - val_loss: 0.6600\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.6076 - loss: 0.6504 - val_accuracy: 0.6194 - val_loss: 0.6514\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 76ms/step - accuracy: 0.6352 - loss: 0.6366 - val_accuracy: 0.6094 - val_loss: 0.6526\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 74ms/step - accuracy: 0.6378 - loss: 0.6287 - val_accuracy: 0.6178 - val_loss: 0.6516\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.6466 - loss: 0.6250 - val_accuracy: 0.6202 - val_loss: 0.6414\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.6453 - loss: 0.6245 - val_accuracy: 0.6367 - val_loss: 0.6358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7d32e6b02010>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the dataset directory\n",
    "dataset_dir = \"/kaggle/input/dog-and-cat-classification-dataset/PetImages\"\n",
    "\n",
    "# Initialize the image data generator with rescaling and validation split\n",
    "image_generator = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "# Create the training data generator\n",
    "train_data = image_generator.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Create the validation data generator\n",
    "validation_data = image_generator.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Define a basic Feedforward Neural Network (BPNN) model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(64, 64, 3)),      # Flatten the input image\n",
    "    Dense(128, activation='relu'),         # First hidden layer\n",
    "    Dense(64, activation='relu'),          # Second hidden layer\n",
    "    Dense(1, activation='sigmoid')         # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and binary cross-entropy loss\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model architecture\n",
    "model.summary()\n",
    "\n",
    "# Train the model using the training and validation data\n",
    "model.fit(train_data, epochs=10, validation_data=validation_data)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPgQ2ge85VdsV+TNvNVJwMo",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
