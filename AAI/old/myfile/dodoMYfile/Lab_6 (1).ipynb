{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","##**Name - Affan**\n","##**Roll No. - 242210001**\n","##**Date - 4/03/2025**"],"metadata":{"id":"p5izmPb_Takj"}},{"source":["import numpy as np\n","\n","class Perceptron:\n","    def __init__(self, input_size, learning_rate=0.1, epochs=100):\n","        self.weights = np.zeros(input_size + 1)\n","        self.learning_rate = learning_rate\n","        self.epochs = epochs\n","\n","    def activation(self, x):\n","        return 1 if x > 0 else 0\n","\n","    def predict(self, inputs):\n","        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n","        return self.activation(summation)\n","\n","    def train(self, X, y):\n","        iterations = 0\n","        for epoch in range(self.epochs):\n","            error_count = 0\n","            for i in range(len(X)):\n","                iterations += 1\n","                prediction = self.predict(X[i])\n","                error = y[i] - prediction\n","                if error != 0:\n","                    self.weights[1:] += self.learning_rate * error * X[i]\n","                    self.weights[0] += self.learning_rate * error\n","            if error_count == 0:\n","                print(f\"Training completed in {epoch + 1} epochs and {iterations} iterations.\")\n","                break\n","        else:\n","            print(f\"Training completed after {self.epochs} epochs and {iterations} iterations.\")\n","\n","# OR Gate dataset\n","X_or = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","y_or = np.array([0, 1, 1, 1])\n","\n","# AND Gate dataset\n","X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","y_and = np.array([0, 0, 0, 1])\n","\n","# Train Perceptron for OR Gate\n","perceptron_or = Perceptron(input_size=2)\n","print(\"Training for OR Gate:\")\n","perceptron_or.train(X_or, y_or)\n","\n","# Train Perceptron for AND Gate\n","perceptron_and = Perceptron(input_size=2)\n","print(\"\\nTraining for AND Gate:\")\n","perceptron_and.train(X_and, y_and)"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U6fLPGGolPRh","outputId":"2a7bce77-5302-4486-8401-8482928e7507"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training for OR Gate:\n","Training completed in 1 epochs and 4 iterations.\n","\n","Training for AND Gate:\n","Training completed in 1 epochs and 4 iterations.\n"]}]},{"cell_type":"markdown","source":["\n","\n","**Limitation of Single Layer perceptron:**\n","\n","A single-layer perceptron can only classify linearly separable data. It works by finding a optimal hyperplane that separates the data points into two classes. But, the XOR problem cannot be solved with a single straight line. We can not draw a line that perfectly separates the points with output '1' from the points with output '0'.\n","\n","**Why the Current Rule Doesn't Apply:**\n","\n","The perceptron's learning rule involves adjusting the weights and bias to minimize classification errors. However, with linearly inseparable data, there will always be some points misclassified, preventing the perceptron from converging to a stable solution. The perceptron update rule aims to find a linear decision boundary, which does not exist for the XOR problem.\n","\n","**Solutions:**\n","\n","Multi-layer Perceptron (MLP):\n","\n","Introduce hidden layers between the input and output layers.\n","Hidden layers allow the network to learn non-linear relationships in the data.\n","The MLP with appropriate activation functions can solve the XOR problem.\n","\n","Feature Engineering:\n","\n","Create new features from the existing ones to make the data linearly separable in a higher-dimensional space.\n","For XOR, we can create a new feature representing the interaction between the two input features.\n","\n"],"metadata":{"id":"9vA0pIvVTZhp"}},{"source":["\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","y = np.array([0, 1, 1, 0])\n","\n","input_size = 2\n","hidden_size = 3\n","output_size = 1\n","\n","W1 = np.random.randn(input_size, hidden_size)\n","b1 = np.zeros(hidden_size)\n","W2 = np.random.randn(hidden_size, output_size)\n","b2 = np.zeros(output_size)\n","\n","learning_rate = 0.1\n","epochs = 10000\n","\n","for epoch in range(epochs):\n","    hidden_layer_input = np.dot(X, W1) + b1\n","    hidden_layer_output = sigmoid(hidden_layer_input)\n","    output_layer_input = np.dot(hidden_layer_output, W2) + b2\n","    output_layer_output = sigmoid(output_layer_input)\n","\n","    error = y - output_layer_output.flatten()\n","\n","    d_output = error * output_layer_output.flatten() * (1 - output_layer_output.flatten())\n","    d_output = d_output.reshape(-1, output_size)\n","    d_hidden = d_output.dot(W2.T) * hidden_layer_output * (1 - hidden_layer_output)\n","\n","    W2 += hidden_layer_output.T.dot(d_output).reshape(hidden_size, output_size) * learning_rate\n","    b2 += np.sum(d_output, axis=0, keepdims=False) * learning_rate\n","    W1 += X.T.dot(d_hidden) * learning_rate\n","    b1 += np.sum(d_hidden, axis=0, keepdims=False) * learning_rate\n","\n","hidden_layer_input = np.dot(X, W1) + b1\n","hidden_layer_output = sigmoid(hidden_layer_input)\n","output_layer_input = np.dot(hidden_layer_output, W2) + b2\n","output_layer_output = sigmoid(output_layer_input)\n","\n","predictions = (output_layer_output > 0.5).astype(int)\n","print(\"Predictions:\", predictions.flatten())"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ilUMJiJ3VGp8","outputId":"9d73aa23-95ed-4355-df63-17a296e59b24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predictions: [0 1 1 0]\n"]}]},{"cell_type":"markdown","source":["**Adapting Perceptron for Regression**\n","\n","In order to use a Perceptron for regression instead of classification, three key changes can be done:\n","\n","**Activation:** Switch from a step function to a linear activation function (f(x) = x). This enables the Perceptron to output continuous values for regression tasks.\n","\n","**Loss Function:** Replacing the Perceptron's error-based update with a regression loss function like Mean Squared Error. This measures the difference between predicted and actual continuous values.\n","\n","**Weight Update:** Continue to use gradient descent, but now aim to minimize the chosen regression loss function to improve predictions.\n","\n","Transforming the Perceptron's output, error measurement, and optimization goal in order to suit the task of predicting continuous values in regression problems."],"metadata":{"id":"XVhGV8RHvQ1g"}},{"source":["\n","class LinearRegressionPerceptron:\n","    def __init__(self, input_size, learning_rate=0.1, epochs=100):\n","        self.weights = np.zeros(input_size + 1)\n","        self.learning_rate = learning_rate\n","        self.epochs = epochs\n","\n","    def predict(self, inputs):\n","        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n","        return summation\n","\n","    def train(self, X, y):\n","        for epoch in range(self.epochs):\n","            for i in range(len(X)):\n","                prediction = self.predict(X[i])\n","                error = y[i] - prediction\n","                self.weights[1:] += self.learning_rate * error * X[i]\n","                self.weights[0] += self.learning_rate * error\n","\n","\n","X = np.array([[1], [2], [3], [4]])\n","y = np.array([2, 4, 6, 8])\n","\n","perceptron = LinearRegressionPerceptron(input_size=1)\n","perceptron.train(X, y)\n","\n","\n","predictions = [perceptron.predict(x) for x in X]\n","print(\"Predictions:\", predictions)"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gL2NR0nlv6UN","outputId":"836ae15e-7bbe-47d5-c241-f7d066ae4cc9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predictions: [2.0004426897546055, 4.0003364442135005, 6.000230198672395, 8.00012395313129]\n"]}]},{"cell_type":"markdown","source":["Perceptron\n","What is a Perceptron?\n","A Perceptron is the simplest type of artificial neural network and is considered the building block of deep learning models.\n","It is a supervised learning algorithm used mainly for binary classification tasks — that is, it classifies data into one of two classes.\n","\n","Introduced by Frank Rosenblatt in 1958, the perceptron mimics the behavior of a single neuron in the human brain.\n","\n","Structure of a Perceptron:\n","A perceptron has three main parts:\n","\n","Input Values (Features):\n","These are the characteristics of the data (e.g., height, weight, color).\n","\n","Weights:\n","Each input feature is multiplied by a corresponding weight, indicating its importance.\n","\n","Activation Function:\n","It processes the weighted sum of inputs and outputs a decision (e.g., 0 or 1).\n","Commonly, a simple step function (threshold function) is used:\n","\n","If weighted sum > threshold → output 1\n","\n","Else → output 0\n","\n","\n","Advantages:\n","Very simple and easy to implement.\n","\n","Useful for problems that are linearly separable (i.e., can be separated with a straight line).\n","\n","Disadvantages:\n","Cannot solve problems that are not linearly separable (e.g., XOR problem).\n","\n","Only suitable for binary classification, not multi-class without modification.\n","\n","Real-Life Examples:\n","Email spam detection (spam or not spam)\n","\n","Predicting whether a customer will buy a product (yes/no)\n","\n","Classifying handwritten digits (in a basic form)\n","\n"],"metadata":{"id":"9fF1IJpJCGT6"}},{"cell_type":"code","source":[],"metadata":{"id":"_6dVMrB0CSd7"},"execution_count":null,"outputs":[]}]}