{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fbv5mSXkLbxf"
   },
   "source": [
    "# **Perceptron**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fNvwk7pLgmY"
   },
   "source": [
    "1. Implement a Perceptron for OR Gate. How many iterations does it take to learn the\n",
    "dataset? (https://medium.com/@stanleydukor/neural-representation-of-and-or-not-xor-and-xnor-logic-gates-perceptron-algorithm-b0275375fea1)\n",
    "2. Implement a Perceptron for AND Gate. How many iterations does it take to learn\n",
    "the dataset? (https://medium.com/@stanleydukor/neural-representation-of-and-or-not-xor-and-xnor-logic-gates-perceptron-algorithm-b0275375fea1)\n",
    "3. Evaluate Perceptron on a Linearly Inseparable Dataset (XOR Problem). Explain\n",
    "what the problem is with the dataset. Why the current rule does not apply to this\n",
    "kind of data? What's your solution to this problem?\n",
    "4. Exploratory problem: Using perceptron to build a linear regression classifier. (https://www.analyticsvidhya.com/blog/2021/11/a-comprehensive-guide-to-linear-regression-with-pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "id": "dfF-nwDdMAg_",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74ynntPxaAwx"
   },
   "source": [
    "**1. Implement a Perceptron for OR Gate. How many iterations does it take to learn the\n",
    "dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "N6hpy0siZqQJ",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "809bbe02-e88b-4d53-9080-29f1a8dcefba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [1.  1.1]\n",
      "Bias: -0.9\n",
      "Iterations to learn: 2\n"
     ]
    }
   ],
   "source": [
    "# Define the OR gate inputs and outputs\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "outputs = np.array([0, 1, 1, 1])\n",
    "\n",
    "# Initialize weights and bias\n",
    "weights = np.array([1.0, 1.0])\n",
    "bias = -1.0\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Training loop\n",
    "iterations = 0\n",
    "while True:\n",
    "    iterations += 1\n",
    "    errors = 0\n",
    "    for i in range(len(inputs)):\n",
    "        # Calculate the weighted sum\n",
    "        weighted_sum = np.dot(inputs[i], weights) + bias\n",
    "        # Apply the activation function (step function)\n",
    "        prediction = 1 if weighted_sum > 0 else 0\n",
    "\n",
    "        # Update weights and bias if prediction is incorrect\n",
    "        if prediction != outputs[i]:\n",
    "            errors += 1\n",
    "            weights += learning_rate * (outputs[i] - prediction) * inputs[i]\n",
    "            bias += learning_rate * (outputs[i] - prediction)\n",
    "\n",
    "    # Check if all predictions are correct\n",
    "    if errors == 0:\n",
    "        break\n",
    "\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Bias:\", bias)\n",
    "print(\"Iterations to learn:\", iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "MjBheD4rqc4I",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "4f74e1e9-d06e-4d1d-b8ab-d21f863eb8be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0], Predicted Output: 0, Actual Output: 0\n",
      "Input: [0 1], Predicted Output: 1, Actual Output: 1\n",
      "Input: [1 0], Predicted Output: 1, Actual Output: 1\n",
      "Input: [1 1], Predicted Output: 1, Actual Output: 1\n"
     ]
    }
   ],
   "source": [
    "# Predictions for OR gate\n",
    "for i in range(len(inputs)):\n",
    "    weighted_sum = np.dot(inputs[i], weights) + bias\n",
    "    prediction = 1 if weighted_sum > 0 else 0\n",
    "    print(f\"Input: {inputs[i]}, Predicted Output: {prediction}, Actual Output: {outputs[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA00TaCndhAZ"
   },
   "source": [
    "**2. Implement a Perceptron for AND Gate. How many iterations does it take to learn the dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "wTtQEe0HdnKh",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "e07b4c4a-1d29-466b-9d37-2508ad4390e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [1. 1.]\n",
      "Bias: -1\n",
      "Iterations to learn: 1\n"
     ]
    }
   ],
   "source": [
    "# Define the AND gate inputs and outputs\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "outputs = np.array([0, 0, 0, 1])\n",
    "\n",
    "# Initialize weights and bias\n",
    "weights = np.array([1.0, 1.0])\n",
    "bias = -1\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Training loop\n",
    "iterations = 0\n",
    "while True:\n",
    "    iterations += 1\n",
    "    errors = 0\n",
    "    for i in range(len(inputs)):\n",
    "        # Calculate the weighted sum\n",
    "        weighted_sum = np.dot(inputs[i], weights) + bias\n",
    "        # Apply the activation function (step function)\n",
    "        prediction = 1 if weighted_sum > 0 else 0\n",
    "\n",
    "        # Update weights and bias if prediction is incorrect\n",
    "        if prediction != outputs[i]:\n",
    "            errors += 1\n",
    "            weights += learning_rate * (outputs[i] - prediction) * inputs[i]\n",
    "            bias += learning_rate * (outputs[i] - prediction)\n",
    "\n",
    "    # Check if all predictions are correct\n",
    "    if errors == 0:\n",
    "        break\n",
    "\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Bias:\", bias)\n",
    "print(\"Iterations to learn:\", iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMwIphmSe78C"
   },
   "source": [
    "**3. Evaluate Perceptron on a Linearly Inseparable Dataset (XOR Problem). Explain what the problem is with the dataset. Why the current rule does not apply to this kind of data? What's your solution to this problem?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "rnM_A0U7fBTp",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "b6923366-c8ca-465e-ab3e-fadf109a1af5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [-1.00000000e-01  1.38777878e-16]\n",
      "Bias: 0.09999999999999987\n",
      "Iterations: 100\n",
      "Perceptron failed to converge within the maximum number of iterations.\n"
     ]
    }
   ],
   "source": [
    "# Define the XOR gate inputs and outputs\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "outputs = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Initialize weights and bias\n",
    "weights = np.array([1.0, 1.0])\n",
    "bias = -1\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Training loop\n",
    "iterations = 0\n",
    "max_iterations = 100  # Setting a maximum number of iterations to prevent infinite loops\n",
    "\n",
    "while iterations < max_iterations:\n",
    "    iterations += 1\n",
    "    errors = 0\n",
    "    for i in range(len(inputs)):\n",
    "        # Calculate the weighted sum\n",
    "        weighted_sum = np.dot(inputs[i], weights) + bias\n",
    "        # Apply the activation function (step function)\n",
    "        prediction = 1 if weighted_sum > 0 else 0\n",
    "\n",
    "        # Update weights and bias if prediction is incorrect\n",
    "        if prediction != outputs[i]:\n",
    "            errors += 1\n",
    "            weights += learning_rate * (outputs[i] - prediction) * inputs[i]\n",
    "            bias += learning_rate * (outputs[i] - prediction)\n",
    "\n",
    "    # Check if all predictions are correct\n",
    "    if errors == 0:\n",
    "        break\n",
    "\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Bias:\", bias)\n",
    "print(\"Iterations:\", iterations)\n",
    "\n",
    "if iterations == max_iterations:\n",
    "    print(\"Perceptron failed to converge within the maximum number of iterations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mF24FVDfiNJ"
   },
   "source": [
    " **Explanation:**\n",
    "\n",
    " The XOR problem is linearly inseparable.  This means you can't draw a single straight line to separate the input points that produce output 0 from those that produce output 1. The Perceptron algorithm, which is based on a linear activation function, can only find linear decision boundaries.  Since XOR requires a non-linear decision boundary, the Perceptron cannot correctly classify all four data points.  It will oscillate between incorrect classifications.\n",
    "\n",
    " **Solution:**\n",
    "\n",
    " To solve the XOR problem, you need a non-linear activation function and/or multiple layers of perceptrons (a neural network).  A simple solution is to use a Multi-Layer Perceptron (MLP) with at least one hidden layer.  The hidden layer introduces non-linearity, enabling the network to learn the XOR function.  Other models like Support Vector Machines with appropriate kernels can also handle this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCLLW5QEhcua"
   },
   "source": [
    "**4. Exploratory problem: Using perceptron to build a linear regression classifier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "id": "_7RgfgJyh-H5",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data for linear regression\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X + 1 + np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "collapsed": true,
    "id": "VCjkIVDNiMkY",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "97afe050-c9f5-4ac2-b52e-b2a34014621a"
   },
   "outputs": [],
   "source": [
    "# visulaize the data\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(X, y, label='Original data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "xeJ1625aiqfp",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "PDebSEjOikqB",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the perceptron model\n",
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "sKVFStC6jTvB",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "model = Perceptron(input_dim, output_dim)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "EE8qdWEKjiCh",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "f3f9b900-f513-4bda-ab46-4f02f935f01f"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "EwSgDCkBjpWh",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predicted = model(X).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "collapsed": true,
    "id": "QQgwiFObjteZ",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "e5c8f5e8-61f9-4f79-b34b-9d672aa42fdb"
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(X.numpy(), y.numpy(), label='Original data')\n",
    "plt.plot(X.numpy(), predicted,color='red', label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
